# Base Configuration for GRPO Training
# This file contains all default settings that can be overridden

# Model Configuration
model:
  name: "Qwen/Qwen2.5-7B-Instruct"
  torch_dtype: "bfloat16"
  low_cpu_mem_usage: true

# Dataset Configuration
data:
  task_name: "leg_counting"
  num_examples: 1024
  seed: 42

# Training Configuration
training:
  output_dir: "./output"
  num_train_epochs: 1
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 2
  learning_rate: 1.0e-6
  lr_scheduler_type: "constant_with_warmup"
  warmup_steps: 100
  num_generations: 8  # Must divide evenly into (batch_size * grad_accumulation)
  temperature: 0.7
  max_grad_norm: 0.2
  logging_steps: 1
  save_steps: 50
  bf16: true
  gradient_checkpointing: false  # Disabled when using LoRA (incompatible with DDP)
  optim: "adamw_torch"  # Regular AdamW optimizer
  use_lora: true  # Use LoRA for parameter-efficient finetuning
  lora_r: 32  # LoRA rank
  lora_alpha: 64  # LoRA alpha scaling

  # GRPO-specific: Reference model and KL penalty
  beta: 0.001  # KL penalty coefficient (0 = no penalty, 0.1 = high penalty)
  sync_ref_model: true  # Whether to sync reference model during training
  ref_model_sync_steps: 10  # How often to sync (in training steps)
  ref_model_mixup_alpha: 0.5  # Mix ratio when syncing (0.5 = 50/50 blend)

# Reward Configuration
rewards:
  - name: "correctness"
    weight: 1.0

# Logging Configuration
logging:
  report_to: "wandb"  # Can be "wandb", "tensorboard", or "none"
  project_name: null  # Will use default if null
  run_name: null  # Will auto-generate if null

# Environment Configuration
environment:
  seed: 42
  set_seed_everywhere: true
