# Qwen 2.5 7B Instruct Model Configuration

model:
  name: "Qwen/Qwen2.5-7B-Instruct"
  torch_dtype: "bfloat16"
  low_cpu_mem_usage: true

training:
  # LoRA allows larger batch sizes (much less memory needed)
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 16
  num_generations: 4

  # Conservative learning rate for 7B
  learning_rate: 1.0e-6 
  max_grad_norm: 0.2
